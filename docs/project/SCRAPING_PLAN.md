# ğŸ•·ï¸ PLAN DE SCRAPING - Textiles Deadstock

**Date** : 27 dÃ©cembre 2025  
**Phase** : Phase 0 - Conception  
**Version** : 1.1  
**DerniÃ¨re MAJ** : 27 dÃ©cembre 2025 - 17:45  

---

## âš ï¸ NOTE IMPORTANTE - NORMALISATION EN ANGLAIS

**DÃ©cision Architecture** : Toutes les donnÃ©es sont normalisÃ©es en **ANGLAIS** dans la database.

**Rationale** :
- âœ… Standard international (textile/mode)
- âœ… Scale futur (sources IT, ES, DE, etc.)
- âœ… Database propre (pas de doublons FR/EN)
- âœ… API-friendly (clients internationaux)

**Traductions** : GÃ©rÃ©es cÃ´tÃ© frontend via i18n layer (FR, EN, ES, IT...)

**Impact Scraping** :
- Sources FR (MLC) : Normalisation FR â†’ EN
- Sources EN (TFS) : Passthrough direct
- Futures sources : Toutes â†’ EN

Voir section **"StratÃ©gie de Normalisation Globale"** pour dÃ©tails complets.

---

## ğŸ¯ Objectif

DÃ©finir la stratÃ©gie complÃ¨te de scraping pour agrÃ©ger les textiles deadstock depuis :
1. **My Little Coupon** (mylittlecoupon.fr) - France
2. **The Fabric Sales** (thefabricsales.com) - Belgique

---

## ğŸ“Š Vue d'Ensemble

### Sources CiblÃ©es MVP (Phase 1)

| Source | URL | Pays | PrioritÃ© | Volume EstimÃ© | FrÃ©quence MAJ |
|--------|-----|------|----------|---------------|---------------|
| **My Little Coupon** | https://mylittlecoupon.fr | ğŸ‡«ğŸ‡· France | P0 | 300-500 refs | Quotidien |
| **The Fabric Sales** | https://thefabricsales.com | ğŸ‡§ğŸ‡ª Belgique | P0 | 200-400 refs | Hebdo (drops) |

### Objectifs MVP (Phase 1)
- **2 sources** opÃ©rationnelles
- **500-900 textiles** en base
- **Taux de succÃ¨s scraping** : >90%
- **Synchronisation** : Quotidienne (MLC) + Hebdomadaire (TFS)
- **Temps scraping total** : <10 minutes par run

### Timeline
- **Semaine 1-2** : DÃ©veloppement scrapers + tests
- **Semaine 3** : Mise en production + monitoring
- **Semaine 4** : Optimisations et debugging

---

## ğŸ” ANALYSE SOURCE 1 : My Little Coupon

### ğŸ“‹ Informations GÃ©nÃ©rales

**URL Base** : https://mylittlecoupon.fr  
**Fondateur** : Elie (2020)  
**ModÃ¨le** : Upcycling fins de sÃ©rie maisons haute couture franÃ§aises/italiennes  
**CommunautÃ©** : 109K followers Instagram, 30K+ clients  
**Format** : Coupons de 3 mÃ¨tres (principalement)  

### ğŸ¯ Pages Cibles

#### Page Principale Collection
**URL** : `https://mylittlecoupon.fr/collections/all`  
**Format** : Grille de produits avec pagination

**DonnÃ©es Ã  Extraire** :
- Nom du textile
- Image principale
- Prix (gÃ©nÃ©ralement pour 3m)
- URL produit dÃ©tail
- DisponibilitÃ© (en stock / rupture)
- Badge "nouveautÃ©" si prÃ©sent

#### Page DÃ©tail Produit
**URL Pattern** : `https://mylittlecoupon.fr/products/[slug]`  
**Exemple** : `https://mylittlecoupon.fr/products/crepe-viscose-blanc-fleurs`

**DonnÃ©es DÃ©taillÃ©es** :
- Nom complet du textile
- Description complÃ¨te
- Composition (ex: "100% viscose", "80% coton 20% polyester")
- Type de matiÃ¨re (viscose, coton, lin, soie, laine, etc.)
- Couleur(s) principale(s)
- Motifs (uni, rayures, fleurs, pois, etc.)
- Provenance (Maison de couture / prÃªt-Ã -porter)
- Longueur du coupon (3m standard, ou 1-2m pour petits coupons)
- Largeur du tissu (en cm)
- Prix
- Images multiples
- Stock disponible

### ğŸ› ï¸ StratÃ©gie Technique MLC

#### Type de Site
- **Plateforme** : Shopify (probable basÃ© sur URL patterns)
- **JavaScript** : Partial rendering (certaines donnÃ©es en JS)
- **API** : Shopify JSON endpoints disponibles

#### Approche RecommandÃ©e : API Shopify + HTML Parsing

**Option A : API Shopify JSON (RecommandÃ©)**
```javascript
// Collection products
GET https://mylittlecoupon.fr/collections/all/products.json
GET https://mylittlecoupon.fr/collections/all/products.json?page=2

// Product details
GET https://mylittlecoupon.fr/products/[handle].json
```

**Avantages** :
- âœ… DonnÃ©es structurÃ©es JSON
- âœ… Pas de parsing HTML complexe
- âœ… Plus rapide et fiable
- âœ… Moins de risque de breakage

**Option B : HTML Scraping (Fallback)**
- Cheerio pour parsing HTML
- SÃ©lecteurs CSS pour extraction
- UtilisÃ© si API JSON bloquÃ©e

#### SÃ©lecteurs CSS (si HTML parsing nÃ©cessaire)

**Page Collection** :
```css
.product-item               /* Chaque produit */
.product-item__title        /* Nom */
.product-item__price        /* Prix */
.product-item__image img    /* Image */
.product-item__link         /* URL dÃ©tail */
.badge                      /* NouveautÃ© / Promo */
```

**Page Produit** :
```css
.product__title             /* Titre */
.product__price             /* Prix */
.product__description       /* Description */
.product__media img         /* Images */
.product-form__variants     /* Variantes si multiples */
```

### ğŸ“… FrÃ©quence de Scraping MLC

**RecommandÃ©** : Quotidien (1x par jour)  
**Heure** : 6h00 AM (avant pic trafic)  
**Raison** : Stock changeant rÃ©guliÃ¨rement, nouveaux coupons frÃ©quents

**StratÃ©gie** :
1. Scraper tous les produits collection principale
2. Pour chaque produit, fetch JSON details
3. DÃ©tecter nouveaux produits vs existants (par source_url unique)
4. Mettre Ã  jour disponibilitÃ© des existants

### ğŸ¨ Normalisation DonnÃ©es MLC

#### Champs Source â†’ Champs DB

| DonnÃ©e MLC | Champ DB | Transformation |
|------------|----------|----------------|
| Product title | `name` | Direct |
| Description | `description` | Direct, strip HTML |
| Composition (dans desc) | `material_type` | Parser (ex: "100% viscose" â†’ "viscose") |
| Composition dÃ©taillÃ©e | `composition` | Parser vers JSONB `{"viscose": 100}` |
| Couleur (dans titre/desc) | `color` | Extract via keywords |
| Motif (dans titre/desc) | `pattern` | Extract via keywords |
| Prix | `price_value` | Parse "49,00 â‚¬" â†’ 49.00 |
| Coupon length (3m) | `quantity_value` | 3 (standard) |
| Unit | `quantity_unit` | "m" |
| Largeur (dans desc) | `width_value` | Parse "laize 140cm" â†’ 140 |
| Image URL | `image_url` | Direct |
| Product URL | `source_url` | Direct (unique key) |
| "Maison de couture" | `supplier_name` | "Maison de couture franÃ§aise" (generic) |

#### Parsing Patterns

**Composition** :
```javascript
// Examples de textes Ã  parser (FR source)
"100% viscose" â†’ {viscose: 100} // EN in DB
"80% coton 20% polyester" â†’ {cotton: 80, polyester: 20} // FRâ†’EN normalized
"CrÃªpe 100% viscose" â†’ {viscose: 100}
```

**Couleur** :
```javascript
// Extract from title or description (FR source)
"VISCOSE BLANC FLEURS" â†’ color: "white" // FRâ†’EN
"Coton rayÃ© bleu marine" â†’ color: "navy blue" // FRâ†’EN
```

**Type de MatÃ©riau** :
```javascript
// Normalize to English
"Coton bio" â†’ material_type: "cotton"
"Soie sauvage" â†’ material_type: "silk"
"Laine mÃ©rinos" â†’ material_type: "wool"
```

**Largeur** :
```javascript
// Parse patterns
"laize 140 cm" â†’ 140
"largeur 150cm" â†’ 150
"laize: 140" â†’ 140
```

### âš ï¸ Challenges & Solutions MLC

#### Challenge 1 : Noms de Maisons Confidentiels
**ProblÃ¨me** : MLC ne rÃ©vÃ¨le pas les noms exacts (clause confidentialitÃ©)  
**Solution** : Utiliser "Maison de couture franÃ§aise/italienne" comme `supplier_name` gÃ©nÃ©rique

#### Challenge 2 : Descriptions VariÃ©es
**ProblÃ¨me** : Format description non standardisÃ©  
**Solution** : 
- Parser avec regex flexibles
- Scorer qualitÃ© des donnÃ©es extraites
- Marquer champs manquants dans `missing_fields[]`

#### Challenge 3 : Stock LimitÃ©
**ProblÃ¨me** : Produits partent vite (Ã©dition limitÃ©e)  
**Solution** :
- Scraping quotidien pour dÃ©tecter ruptures
- Marquer `available = false` si rupture dÃ©tectÃ©e
- Garder l'historique (ne pas supprimer)

---

## ğŸ” ANALYSE SOURCE 2 : The Fabric Sales

### ğŸ“‹ Informations GÃ©nÃ©rales

**URL Base** : https://thefabricsales.com  
**Pays** : Belgique  
**ModÃ¨le** : Drops hebdomadaires de deadstock designers europÃ©ens  
**Format** : Vente exclusivitÃ© limitÃ©e dans le temps  
**Positionnement** : Premium Belgian fabric shop  

### ğŸ¯ Pages Cibles

#### Page Shop/Collection
**URL** : `https://thefabricsales.com/collections/all`  
**Format** : Grille produits avec filtres

**DonnÃ©es Listing** :
- Nom fabric
- Image
- Prix
- Designer/Brand si mentionnÃ©
- URL produit

#### Page Produit DÃ©tail
**URL Pattern** : `https://thefabricsales.com/products/[slug]`

**DonnÃ©es DÃ©taillÃ©es** :
- Nom complet
- Description
- Composition (ex: "100% cotton", "silk blend")
- Type de fabric (cotton, silk, wool, linen, etc.)
- Couleur(s)
- Pattern (prints, solid, stripes)
- Designer/Brand origin
- QuantitÃ© disponible (en mÃ¨tres)
- Largeur (width en cm)
- Prix par mÃ¨tre
- Prix total si quantitÃ© fixe
- Care instructions
- Images multiples
- Stock status

### ğŸ› ï¸ StratÃ©gie Technique TFS

#### Type de Site
- **Plateforme** : Shopify (basÃ© sur structure URL)
- **JavaScript** : Partial
- **API** : Shopify JSON endpoints probables

#### Approche : API Shopify JSON

**Endpoints** :
```javascript
// Collections
GET https://thefabricsales.com/collections/all/products.json

// Product details
GET https://thefabricsales.com/products/[handle].json
```

**MÃªme logique que MLC** : Utiliser API JSON Shopify pour donnÃ©es structurÃ©es

#### SÃ©lecteurs CSS Fallback

```css
.product-card               /* Produit listing */
.product-card__title        /* Nom */
.product-card__price        /* Prix */
.product__title             /* Titre dÃ©tail */
.product__description       /* Description */
```

### ğŸ“… FrÃ©quence de Scraping TFS

**RecommandÃ©** : Hebdomadaire + Event-based  
**Jour** : Lundi ou aprÃ¨s annonce drop (monitoring Instagram)  
**Raison** : Drops hebdomadaires, stock renouvelÃ© par vagues

**StratÃ©gie** :
1. **Scraping hebdomadaire rÃ©gulier** : Tous les lundis 7h AM
2. **Scraping event-based** : Si dÃ©tection nouveau drop (via RSS/social)
3. **Scraping de vÃ©rification** : Mi-semaine pour mÃ j stock

### ğŸ¨ Normalisation DonnÃ©es TFS

#### Champs Source â†’ Champs DB

| DonnÃ©e TFS | Champ DB | Transformation |
|------------|----------|----------------|
| Product name | `name` | Direct |
| Description | `description` | Strip HTML |
| Fabric type | `material_type` | Extract (cotton, silk, wool, linen) |
| Composition % | `composition` | Parse vers JSONB |
| Color | `color` | Extract from title/desc |
| Pattern | `pattern` | Extract keywords |
| Price | `price_value` | Parse currency |
| Quantity (meters) | `quantity_value` | Parse number |
| Width | `width_value` | Parse "150cm wide" |
| Designer mention | `supplier_name` | Extract si prÃ©sent, sinon "European designer" |
| Image | `image_url` | Direct |
| URL | `source_url` | Direct (unique) |

#### Parsing Patterns TFS

**Composition** :
```javascript
// Already in English - passthrough
"100% Cotton" â†’ {cotton: 100}
"Silk Blend 70% silk 30% cotton" â†’ {silk: 70, cotton: 30}
```

**Couleur** :
```javascript
// Already in English - passthrough
"Navy Blue Cotton" â†’ color: "navy blue"
"White Linen" â†’ color: "white"
```

**Type de MatÃ©riau** :
```javascript
// Already in English - passthrough
"Pure Silk" â†’ material_type: "silk"
"Cotton Canvas" â†’ material_type: "cotton"
```

**QuantitÃ©** :
```javascript
"5 meters available" â†’ 5
"Sold per meter" â†’ NULL (Ã  calculer)
```

**Largeur** :
```javascript
"150cm wide" â†’ 150
"Width: 140cm" â†’ 140
```

### âš ï¸ Challenges & Solutions TFS

#### Challenge 1 : Drops Hebdomadaires
**ProblÃ¨me** : Stock renouvelÃ© par vagues, timing important  
**Solution** :
- Monitoring Instagram/Newsletter pour annonces drops
- Scraping immÃ©diat aprÃ¨s drop
- Webhook ou notification pour trigger scraping

#### Challenge 2 : Prix Variables
**ProblÃ¨me** : Parfois prix au mÃ¨tre, parfois prix total  
**Solution** :
- DÃ©tecter format prix
- Calculer `price_per_unit` si possible
- Documenter dans `price_per_unit_label`

#### Challenge 3 : Stock LimitÃ© Drops
**ProblÃ¨me** : Produits partent trÃ¨s vite aprÃ¨s drops  
**Solution** :
- Scraping rapide aprÃ¨s annonce drop
- Marquer `available = false` rapidement si rupture
- Historique gardÃ© pour analytics

---

## ğŸ”„ STRATÃ‰GIE DE NORMALISATION GLOBALE

### ğŸ¯ Objectif Normalisation

Transformer donnÃ©es hÃ©tÃ©rogÃ¨nes (sources multilingues) en format unifiÃ© **ANGLAIS** dans `deadstock.textiles`.

**DÃ©cision Architecture** : 
- **Database** : DonnÃ©es normalisÃ©es en **anglais** (immuable, standard international)
- **Interface** : Traductions via i18n layer selon locale utilisateur (FR, EN, ES, IT, etc.)
- **Avantages** : Scale international, pas de doublons, API-friendly

### ğŸ“Š Mapping Terminologie â†’ ANGLAIS

#### Types de MatÃ©riaux (FR/EN â†’ Standard ANGLAIS)

| FranÃ§ais (MLC) | Anglais (TFS) | **Standard DB (EN)** | Category |
|----------------|---------------|---------------------|----------|
| Coton | Cotton | **cotton** | natural |
| Soie | Silk | **silk** | natural |
| Laine | Wool | **wool** | natural |
| Lin | Linen | **linen** | natural |
| Viscose | Viscose | **viscose** | artificial |
| Polyester | Polyester | **polyester** | synthetic |
| Nylon | Nylon | **nylon** | synthetic |
| Denim/Jean | Denim | **denim** | specific |
| Velours | Velvet | **velvet** | texture |
| Dentelle | Lace | **lace** | texture |

#### Couleurs (FR/EN â†’ Standard ANGLAIS)

| FranÃ§ais | Anglais | **Standard DB (EN)** |
|----------|---------|---------------------|
| Blanc | White | **white** |
| Noir | Black | **black** |
| Rouge | Red | **red** |
| Bleu | Blue | **blue** |
| Bleu marine | Navy | **navy blue** |
| Rose | Pink | **pink** |
| Vert | Green | **green** |
| Jaune | Yellow | **yellow** |
| Violet/Mauve | Purple | **purple** |
| Beige | Beige | **beige** |
| Multicolore | Multicolor | **multicolor** |

#### Motifs (FR/EN â†’ Standard ANGLAIS)

| FranÃ§ais | Anglais | **Standard DB (EN)** |
|----------|---------|---------------------|
| Uni | Solid | **solid** |
| Rayures/RayÃ© | Stripes | **stripes** |
| Fleurs/Fleuri | Floral | **floral** |
| Pois | Dots/Polka | **dots** |
| Carreaux | Checks | **checks** |
| GÃ©omÃ©trique | Geometric | **geometric** |
| Abstrait | Abstract | **abstract** |
| ImprimÃ© animal | Animal print | **animal print** |

### ğŸŒ Internationalisation (i18n)

#### Architecture Traductions

**Database** : DonnÃ©es en anglais (immuable)
```json
{
  "material_type": "cotton",
  "color": "navy blue",
  "pattern": "floral"
}
```

**Frontend** : Traductions via next-intl ou i18next
```typescript
// locales/fr.json
{
  "materials": {
    "cotton": "Coton",
    "silk": "Soie",
    "wool": "Laine"
  },
  "colors": {
    "white": "Blanc",
    "black": "Noir",
    "navy blue": "Bleu marine"
  },
  "patterns": {
    "floral": "Fleuri",
    "stripes": "Rayures",
    "solid": "Uni"
  }
}

// locales/es.json
{
  "materials": {
    "cotton": "AlgodÃ³n",
    "silk": "Seda",
    "wool": "Lana"
  }
  // ...
}
```

**Utilisation** :
```typescript
// Component
const { t } = useTranslation();

<div>
  <p>{t(`materials.${textile.material_type}`)}</p>
  <p>{t(`colors.${textile.color}`)}</p>
</div>

// Affichage FR: "Coton, Bleu marine"
// Affichage EN: "Cotton, Navy blue"
// Affichage ES: "AlgodÃ³n, Azul marino"
```

#### Langues SupportÃ©es (Roadmap)

**Phase 1 (MVP)** : 
- ğŸ‡«ğŸ‡· FranÃ§ais (principale)
- ğŸ‡¬ğŸ‡§ Anglais

**Phase 3-4** :
- ğŸ‡ªğŸ‡¸ Espagnol
- ğŸ‡®ğŸ‡¹ Italien
- ğŸ‡©ğŸ‡ª Allemand

**Phase 7+** :
- Autres langues selon demande marchÃ©

### ğŸ”§ Fonctions Normalisation

#### 1. Parse Composition
```typescript
function parseComposition(text: string, sourceLang: 'fr' | 'en'): Record<string, number> {
  // "100% viscose" â†’ {viscose: 100}
  // "80% cotton 20% polyester" â†’ {cotton: 80, polyester: 20}
  // "80% coton 20% polyester" â†’ {cotton: 80, polyester: 20} (normalized to EN)
  
  const composition: Record<string, number> = {};
  const pattern = /(\d+)\s*%\s*([a-zA-ZÃ©Ã¨Ãª]+)/gi;
  
  let match;
  while ((match = pattern.exec(text)) !== null) {
    const percentage = parseInt(match[1]);
    const material = normalizeMaterial(match[2], sourceLang); // Returns EN
    composition[material] = percentage;
  }
  
  return composition;
}

function normalizeMaterial(material: string, sourceLang: 'fr' | 'en'): string {
  const materialMap = {
    fr: {
      'coton': 'cotton',
      'soie': 'silk',
      'laine': 'wool',
      'lin': 'linen',
      'viscose': 'viscose',
      'polyester': 'polyester',
      // ... complete mapping
    },
    en: {
      'cotton': 'cotton',
      'silk': 'silk',
      'wool': 'wool',
      // ... passthrough for EN
    }
  };
  
  const normalized = material.toLowerCase().trim();
  return materialMap[sourceLang][normalized] || normalized;
}
```

#### 2. Extract Material Type
```typescript
function extractMaterialType(
  name: string, 
  description: string, 
  composition: any,
  sourceLang: 'fr' | 'en'
): string {
  // Returns material in ENGLISH
  
  if (composition) {
    // Return material with highest %
    const sorted = Object.entries(composition).sort((a, b) => b[1] - a[1]);
    if (sorted.length > 0) return sorted[0][0]; // Already in EN from parseComposition
  }
  
  // Search keywords in name/description and normalize to EN
  const text = (name + ' ' + description).toLowerCase();
  const materials = sourceLang === 'fr' 
    ? ['coton', 'soie', 'laine', 'lin', 'viscose', 'polyester']
    : ['cotton', 'silk', 'wool', 'linen', 'viscose', 'polyester'];
  
  for (const mat of materials) {
    if (text.includes(mat)) {
      return normalizeMaterial(mat, sourceLang); // Returns EN
    }
  }
  
  return 'unknown'; // To review manually
}
```

#### 3. Extract Color
```typescript
function extractColor(text: string, sourceLang: 'fr' | 'en'): string | null {
  // Returns color in ENGLISH
  
  const colorKeywords = {
    'white': {
      fr: ['blanc', 'ivory', 'Ã©cru'],
      en: ['white', 'ivory', 'ecru']
    },
    'black': {
      fr: ['noir'],
      en: ['black']
    },
    'red': {
      fr: ['rouge', 'bordeaux'],
      en: ['red', 'bordeaux']
    },
    'blue': {
      fr: ['bleu'],
      en: ['blue']
    },
    'navy blue': {
      fr: ['marine', 'bleu marine'],
      en: ['navy', 'navy blue']
    },
    'pink': {
      fr: ['rose'],
      en: ['pink']
    },
    'green': {
      fr: ['vert'],
      en: ['green']
    },
    // ... complete mapping
  };
  
  const normalized = text.toLowerCase();
  
  for (const [standardEN, variants] of Object.entries(colorKeywords)) {
    const keywords = variants[sourceLang];
    for (const keyword of keywords) {
      if (normalized.includes(keyword)) return standardEN; // Returns EN
    }
  }
  
  return null;
}
```

#### 4. Extract Pattern
```typescript
function extractPattern(text: string, sourceLang: 'fr' | 'en'): string | null {
  // Returns pattern in ENGLISH
  
  const patternKeywords = {
    'solid': {
      fr: ['uni'],
      en: ['solid']
    },
    'stripes': {
      fr: ['rayÃ©', 'rayures'],
      en: ['stripes', 'striped']
    },
    'floral': {
      fr: ['fleurs', 'fleuri', 'floral'],
      en: ['floral', 'flower', 'flowers']
    },
    'dots': {
      fr: ['pois'],
      en: ['dots', 'polka']
    },
    // ... complete mapping
  };
  
  const normalized = text.toLowerCase();
  
  for (const [standardEN, variants] of Object.entries(patternKeywords)) {
    const keywords = variants[sourceLang];
    for (const keyword of keywords) {
      if (normalized.includes(keyword)) return standardEN;
    }
  }
  
  return null;
}
```

#### 4. Calculate Data Quality Score
```typescript
function calculateQualityScore(textile: any): number {
  let score = 0;
  const weights = {
    name: 10,
    description: 10,
    material_type: 15,
    composition: 15,
    color: 10,
    quantity_value: 10,
    price_value: 10,
    image_url: 10,
    width_value: 5,
    supplier_name: 5
  };
  
  for (const [field, weight] of Object.entries(weights)) {
    if (textile[field]) score += weight;
  }
  
  return score; // 0-100
}
```

### ğŸ“‹ UnitÃ©s StandardisÃ©es

#### QuantitÃ©
- **Standard** : mÃ¨tres (m)
- **Conversions** :
  - yards â†’ m : `value * 0.9144`
  - feet â†’ m : `value * 0.3048`
  - cm â†’ m : `value / 100`

#### Largeur
- **Standard** : centimÃ¨tres (cm)
- **Conversions** :
  - inches â†’ cm : `value * 2.54`
  - m â†’ cm : `value * 100`

#### Prix
- **Standard** : EUR (â‚¬)
- **Conversions** (Phase 2+) :
  - USD â†’ EUR : API exchange rate
  - GBP â†’ EUR : API exchange rate

---

## ğŸ¤– ARCHITECTURE SCRAPING

### ğŸ“ Structure Fichiers

```
scripts/
â””â”€â”€ scrapers/
    â”œâ”€â”€ common/
    â”‚   â”œâ”€â”€ normalizer.ts       # Fonctions normalisation
    â”‚   â”œâ”€â”€ parser.ts           # Parsing compositions, colors
    â”‚   â””â”€â”€ supabase.ts         # Client Supabase
    â”‚
    â”œâ”€â”€ my-little-coupon/
    â”‚   â”œâ”€â”€ scraper.ts          # Logic scraping MLC
    â”‚   â”œâ”€â”€ config.ts           # URLs, selectors
    â”‚   â””â”€â”€ test.ts             # Tests unitaires
    â”‚
    â”œâ”€â”€ the-fabric-sales/
    â”‚   â”œâ”€â”€ scraper.ts          # Logic scraping TFS
    â”‚   â”œâ”€â”€ config.ts           # URLs, selectors
    â”‚   â””â”€â”€ test.ts             # Tests unitaires
    â”‚
    â””â”€â”€ orchestrator.ts         # Run all scrapers
```

### ğŸ”„ Workflow Scraping

```mermaid
graph TD
    A[Cron Trigger] --> B[Orchestrator]
    B --> C[Log Start - deadstock.scraping_logs]
    C --> D{For Each Source}
    
    D --> E[Scraper MLC]
    D --> F[Scraper TFS]
    
    E --> G[Fetch Products JSON]
    F --> G
    
    G --> H[For Each Product]
    H --> I[Normalize Data]
    I --> J[Calculate Quality Score]
    J --> K{Product Exists?}
    
    K -->|No| L[Insert New]
    K -->|Yes| M[Update Existing]
    
    L --> N[Log Success]
    M --> N
    
    N --> O[Update scraping_logs]
    O --> P[Send Notification]
```

### ğŸ¯ Pseudo-Code Scraper

```typescript
async function scrapeMLC() {
  const log = await startScrapingLog('my_little_coupon');
  
  try {
    // 1. Fetch all products
    const products = await fetchMLCProducts();
    log.items_found = products.length;
    
    // 2. For each product
    for (const product of products) {
      try {
        // 2a. Normalize data
        const normalized = normalizeMLCProduct(product);
        
        // 2b. Calculate quality score
        normalized.data_quality_score = calculateQualityScore(normalized);
        
        // 2c. Upsert in DB (by source_url)
        await supabase
          .from('deadstock.textiles')
          .upsert(normalized, { onConflict: 'source_url' });
        
        log.items_new++; // or items_updated
        
      } catch (err) {
        log.items_failed++;
        console.error('Failed product:', product.id, err);
      }
    }
    
    // 3. Complete log
    await completeScrapingLog(log, 'completed');
    
  } catch (err) {
    await completeScrapingLog(log, 'failed', err.message);
  }
}
```

### â±ï¸ Scheduling (Cron Jobs)

#### Vercel Cron (RecommandÃ© pour MVP)

**vercel.json** :
```json
{
  "crons": [
    {
      "path": "/api/cron/scrape-mlc",
      "schedule": "0 6 * * *"
    },
    {
      "path": "/api/cron/scrape-tfs",
      "schedule": "0 7 * * 1"
    }
  ]
}
```

**API Route** : `app/api/cron/scrape-mlc/route.ts`
```typescript
export async function GET(request: Request) {
  // Verify cron secret
  const authHeader = request.headers.get('authorization');
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return new Response('Unauthorized', { status: 401 });
  }
  
  // Run scraper
  await scrapeMLC();
  
  return Response.json({ success: true });
}
```

#### GitHub Actions (Alternative)

```yaml
# .github/workflows/scrape-daily.yml
name: Daily Scraping
on:
  schedule:
    - cron: '0 6 * * *'  # 6 AM daily
  workflow_dispatch:       # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
      - run: npm ci
      - run: npm run scrape:all
```

---

## âš ï¸ GESTION ERREURS & MONITORING

### ğŸš¨ Types d'Erreurs

#### 1. Network Errors
**SymptÃ´mes** : Timeout, DNS failure, connection refused  
**Handling** :
- Retry 3x avec backoff exponentiel
- Log dans `scraping_logs.error_details`
- Alert si >3 Ã©checs consÃ©cutifs

#### 2. Parsing Errors
**SymptÃ´mes** : Selector not found, invalid JSON  
**Handling** :
- Log produit problÃ©matique
- Continuer avec autres produits
- Alert si >10% Ã©checs

#### 3. Data Quality Issues
**SymptÃ´mes** : Champs manquants, formats invalides  
**Handling** :
- Accepter avec `data_quality_score` bas
- Marquer `missing_fields[]`
- Review manuel si score <30

### ğŸ“Š Monitoring & Alerting

#### MÃ©triques Ã  Tracker
- Taux de succÃ¨s par source (>90% attendu)
- DurÃ©e scraping (<10 min attendu)
- Nouveaux produits par jour
- Produits devenus indisponibles
- Score qualitÃ© moyen

#### Dashboard Monitoring (Phase 2)

```sql
-- Vue stats scraping
CREATE VIEW scraping_stats_7days AS
SELECT 
  source_platform,
  COUNT(*) as runs,
  AVG(items_found) as avg_items,
  AVG(items_failed) as avg_failures,
  AVG(duration_seconds) as avg_duration
FROM deadstock.scraping_logs
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY source_platform;
```

#### Alerting Rules
- âœ… Si scraping Ã©choue 2x consÃ©cutifs â†’ Email
- âœ… Si taux Ã©chec >20% â†’ Email
- âœ… Si durÃ©e >20 min â†’ Email
- âœ… Si 0 nouveaux produits pendant 7 jours â†’ Warning

---

## ğŸ§ª TESTING STRATEGY

### Unit Tests

```typescript
// tests/normalizer.test.ts
describe('parseComposition', () => {
  it('parses simple composition', () => {
    expect(parseComposition('100% cotton'))
      .toEqual({ cotton: 100 });
  });
  
  it('parses mixed composition', () => {
    expect(parseComposition('80% coton 20% polyester'))
      .toEqual({ coton: 80, polyester: 20 });
  });
});

describe('extractColor', () => {
  it('extracts color from French text', () => {
    expect(extractColor('Viscose Blanc Fleurs'))
      .toBe('blanc');
  });
  
  it('extracts color from English text', () => {
    expect(extractColor('Navy Blue Cotton'))
      .toBe('bleu marine');
  });
});
```

### Integration Tests

```typescript
// tests/scraper-mlc.test.ts
describe('MLC Scraper', () => {
  it('fetches products from MLC', async () => {
    const products = await fetchMLCProducts();
    expect(products.length).toBeGreaterThan(0);
  });
  
  it('normalizes MLC product correctly', async () => {
    const raw = await fetchMLCProduct('test-product-slug');
    const normalized = normalizeMLCProduct(raw);
    
    expect(normalized.name).toBeDefined();
    expect(normalized.source_url).toContain('mylittlecoupon.fr');
    expect(normalized.source_platform).toBe('my_little_coupon');
  });
});
```

### Manual Testing Checklist

- [ ] Scraper MLC fetch >100 products
- [ ] Scraper TFS fetch >100 products
- [ ] Parsing composition fonctionne (10 exemples)
- [ ] Extraction couleur fonctionne (10 exemples)
- [ ] Images URLs valides
- [ ] Upsert DB sans doublon (test source_url unique)
- [ ] Logs scraping crÃ©Ã©s correctement
- [ ] Quality scores calculÃ©s (distribution 30-100)

---

## ğŸ“… TIMELINE IMPLÃ‰MENTATION

### Semaine 1 : Scraper MLC
**Jours 1-2** :
- [ ] Setup structure projet scrapers
- [ ] ImplÃ©menter `common/normalizer.ts`
- [ ] ImplÃ©menter `common/parser.ts`

**Jours 3-4** :
- [ ] DÃ©velopper scraper MLC
- [ ] Tests avec 10-20 produits
- [ ] Debugging et ajustements

**Jour 5** :
- [ ] Tests complets (tous produits MLC)
- [ ] Fix bugs identifiÃ©s
- [ ] Documentation code

### Semaine 2 : Scraper TFS + Orchestration
**Jours 1-2** :
- [ ] DÃ©velopper scraper TFS
- [ ] Adaptation normalizer pour EN
- [ ] Tests avec 10-20 produits

**Jours 3-4** :
- [ ] Orchestrator pour run both scrapers
- [ ] Setup Vercel Cron ou GitHub Actions
- [ ] Tests end-to-end

**Jour 5** :
- [ ] Monitoring et logging
- [ ] Documentation complÃ¨te
- [ ] Dry-run en prod

### Semaine 3 : Production & Monitoring
**Jours 1-3** :
- [ ] Deploy en production
- [ ] Premier scraping complet
- [ ] Validation donnÃ©es en DB
- [ ] VÃ©rification quality scores

**Jours 4-5** :
- [ ] Monitoring 24h
- [ ] Ajustements si nÃ©cessaire
- [ ] Documentation issues rencontrÃ©es

### Semaine 4 : Optimisations
- [ ] AmÃ©liorer performance si >10min
- [ ] AmÃ©liorer quality scores si <70 avg
- [ ] Ajouter alerting
- [ ] PrÃ©parer Phase 2 (nouvelles sources)

---

## ğŸ“‹ CHECKLIST SCRAPING PRODUCTION

### PrÃ©-Lancement
- [ ] Code scrapers testÃ© avec >100 produits par source
- [ ] Normalizer testÃ© avec compositions variÃ©es
- [ ] Database schema `deadstock.textiles` crÃ©Ã©
- [ ] Service role Supabase key configurÃ©e
- [ ] Cron jobs configurÃ©s (Vercel ou GitHub)
- [ ] Monitoring dashboard accessible
- [ ] Alerting configurÃ© (email)
- [ ] Documentation scraper complÃ¨te

### Post-Lancement (J+1)
- [ ] VÃ©rifier scraping_logs (status='completed')
- [ ] Compter textiles en DB (SELECT COUNT(*))
- [ ] VÃ©rifier quality scores (SELECT AVG(data_quality_score))
- [ ] VÃ©rifier images chargent
- [ ] Tester recherche full-text
- [ ] Identifier produits avec missing_fields

### Maintenance Continue
- [ ] Review logs hebdomadaire
- [ ] Ajuster parsers si nouveaux formats
- [ ] Ajouter tests pour edge cases
- [ ] AmÃ©liorer quality scoring
- [ ] Surveiller changes structure sites sources

---

## ğŸš€ Ã‰VOLUTIONS FUTURES

### Phase 2-3 (Court Terme)
- Ajouter 3-5 nouvelles sources (Nona Source, Beglarian, etc.)
- AmÃ©liorer normalisation avec ML (auto-detect couleurs)
- Scraping incrÃ©mental (seulement nouveautÃ©s)
- Cache HTTP pour optimiser vitesse

### Phase 4-6 (Moyen Terme)
- NÃ©gocier accÃ¨s API direct avec sources
- Webhooks pour sync temps rÃ©el
- Scraping images et re-host sur Supabase Storage
- OCR sur images pour extraire infos manquantes

### Phase 7+ (Long Terme)
- Computer vision pour dÃ©tecter motifs/couleurs auto
- Scraping reviews/ratings clients
- Scraping related products (suggestions)
- International sources (US, Asia)

---

## ğŸ”— RESSOURCES & RÃ‰FÃ‰RENCES

### Documentation Technique
- [Cheerio Documentation](https://cheerio.js.org/)
- [Puppeteer Documentation](https://pptr.dev/)
- [Supabase JS Client](https://supabase.com/docs/reference/javascript)
- [Vercel Cron Jobs](https://vercel.com/docs/cron-jobs)

### Best Practices Scraping
- Respecter robots.txt
- Rate limiting (max 1 req/sec)
- User-Agent headers appropriÃ©s
- Respect des ToS des sites

### LÃ©galitÃ©
- Scraping donnÃ©es publiques = gÃ©nÃ©ralement OK
- Respecter RGPD (pas de donnÃ©es personnelles)
- Usage commercial dÃ©rivÃ© = vÃ©rifier ToS

---

## ğŸ“ NOTES IMPORTANTES

### Ã‰thique Scraping
- âœ… On agrÃ¨ge des donnÃ©es publiques
- âœ… On cite toujours la source (`source_url`)
- âœ… On redirige users vers source originale
- âœ… On ne stocke pas de donnÃ©es personnelles
- âœ… On respecte robots.txt

### Monitoring Sites Sources
- Suivre changements structure HTML (GitHub watch)
- Tester scrapers rÃ©guliÃ¨rement
- Avoir fallback si source down
- Documenter changes dans CHANGELOG

### Performance
- ParallÃ©liser scrapers (MLC + TFS simultanÃ©s)
- Batch inserts (100 products Ã  la fois)
- Caching results 24h si re-run nÃ©cessaire
- Monitoring durÃ©e (alert si >15min)

---

**Prochaine Action** : DÃ©velopper scraper MLC (Semaine 1 Phase 1) ğŸš€

**Auteur** : Thomas  
**ValidÃ©** : 27 dÃ©cembre 2025  
**Status** : âœ… Plan finalisÃ©, prÃªt pour implÃ©mentation
